{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54324780-c227-431b-8542-bda312a406bd",
   "metadata": {},
   "source": [
    "## Text-To-Text Transfer Transformer (T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a12734-6d96-485c-b3a8-49c82970d93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch. utils.data import Dataset, DataLoader\n",
    "# PyTorch Lightning is built on top of ordinary (vanilla) PyTorch. The purpose of Lightning is \n",
    "#to provide a research framework that allows for fast experimentation and scalability, which it\n",
    "#achieves via an OOP approach that removes boilerplate and hardware-reference code.\n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from termcolor import colored\n",
    "import textwrap\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5TokenizerFast as T5Tokenizer\n",
    ")\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06576669-bbfb-4e0d-9e49-d668d412de01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "rcParams['figure.figsize'] = 16, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b20792-d85e-40f0-abac-fea52600e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e872c-c944-4c2d-8ca9-56db62731d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from Kaggle news_summary.csv\n",
    "df = pd.read_csv(\"data/news_summary.csv\", encoding=\"latin-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b34555-0972-4891-8493-2ecf4534e94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text', 'ctext']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e151b6-1b20-4c27-a269-430b278ad420",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['summary', 'text']\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57507aef-9ad6-4630-9233-5a0c4a028e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e14964-49ec-4c90-850e-6c83e7a459d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.1)\n",
    "print(f\"Shape of the Train Set: {train_df.shape}\\nShape of the Test Set: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05201cc2-14bc-4a63-ae31-430be8c95633",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, text_max_token_len=512, summary_max_token_len=128):\n",
    "        \"\"\"\n",
    "        A dataset that represents news articles and their respective summaries.\n",
    "\n",
    "        Args:\n",
    "        - data (pd.DataFrame): The data that contains the news articles and their summaries.\n",
    "        - tokenizer (transformers.tokenization_*) : The tokenizer used to tokenize the text and summary.\n",
    "        - text_max_token_len (int, optional): The maximum length of the text in terms of tokens. Defaults to 512.\n",
    "        - summary_max_token_len (int, optional): The maximum length of the summary in terms of tokens. Defaults to 128.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.text_max_token_len = text_max_token_len\n",
    "        self.summary_max_token_len = summary_max_token_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        - The number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get a sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "        - index (int): The index of the sample to get.\n",
    "\n",
    "        Returns:\n",
    "        - A dictionary that contains the following:\n",
    "            - text (str): The original text of the news article.\n",
    "            - summary (str): The summary of the news article.\n",
    "            - text_input_ids (torch.Tensor): The input IDs of the text after tokenization.\n",
    "            - text_attention_mask (torch.Tensor): The attention mask of the text after tokenization.\n",
    "            - labels (torch.Tensor): The input IDs of the summary after tokenization.\n",
    "            - labels_attention_mask (torch.Tensor): The attention mask of the summary after tokenization.\n",
    "        \"\"\"\n",
    "        data_row = self.data.iloc[index]\n",
    "        text = data_row[\"text\"]\n",
    "\n",
    "        # Encode the text\n",
    "        text_encoding = self.tokenizer(\n",
    "            text, \n",
    "            max_length=self.text_max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Encode the summary\n",
    "        summary_encoding = self.tokenizer(\n",
    "            data_row[\"summary\"], \n",
    "            max_length=self.summary_max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Modify the labels so that the model knows which tokens to predict\n",
    "        labels = summary_encoding['input_ids']\n",
    "        labels[labels == 0] = -100\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'summary': data_row['summary'],\n",
    "            'text_input_ids': text_encoding['input_ids'].flatten(),\n",
    "            'text_attention_mask': text_encoding['attention_mask'].flatten(),\n",
    "            'labels': labels.flatten(),\n",
    "            'labels_attention_mask': summary_encoding[\"attention_mask\"].flatten()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108e97d8-cc0e-4db0-8b32-f6cbdfa8c25f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a15d1c3-3aa9-4154-a1ad-035302072c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 train_df,\n",
    "                 test_df,\n",
    "                 tokenizer,\n",
    "                 batch_size=16,\n",
    "                 text_max_token_len=152,\n",
    "                 summary_max_token_len=128):\n",
    "        \"\"\"\n",
    "        Initializes the NewsDataModule.\n",
    "        \n",
    "        Args:\n",
    "        - train_df (pandas.DataFrame): the training dataset\n",
    "        - test_df (pandas.DataFrame): the testing dataset\n",
    "        - tokenizer (transformers.PreTrainedTokenizer): the tokenizer to be used\n",
    "        - batch_size (int): the batch size\n",
    "        - text_max_token_len (int): the maximum number of tokens for the text\n",
    "        - summary_max_token_len (int): the maximum number of tokens for the summary\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_max_token_len = text_max_token_len\n",
    "        self.summary_max_token_len = summary_max_token_len\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"\n",
    "        Sets up the dataset.\n",
    "        \"\"\"\n",
    "        self.train_dataset = NewsDataset(\n",
    "            self.train_df,\n",
    "            self.tokenizer,\n",
    "            self.text_max_token_len,\n",
    "            self.summary_max_token_len)\n",
    "        \n",
    "        self.test_dataset = NewsDataset(\n",
    "            self.test_df,\n",
    "            self.tokenizer,\n",
    "            self.text_max_token_len,\n",
    "            self.summary_max_token_len)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Returns the DataLoader for the training set.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        \"\"\"\n",
    "        Returns the DataLoader for the testing set.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        \"\"\"\n",
    "        Returns the DataLoader for the validation set, which is the same as the testing set.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def710c4-05eb-4e12-b1b1-34b382ec70d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca79d29-9c05-491d-b946-3640d8ac0f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55753b8-ecd1-41c0-8bce-3558be27c18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_token_counts = [len(tokenizer.encode(row[\"text\"])) for _, row in train_df.iterrows()]\n",
    "summary_token_counts = [len(tokenizer.encode(row[\"summary\"])) for _, row in train_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6767a692-f783-4520-abe0-37f767847ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "sns.histplot(text_token_counts, ax=ax1, color='blue', alpha=0.7)\n",
    "ax1.set_title(\"Distribution of Text Token Counts\", fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel(\"Number of Tokens\", fontsize=12)\n",
    "ax1.set_ylabel(\"Frequency\", fontsize=12)\n",
    "ax1.grid(axis='y', alpha=0.5)\n",
    "\n",
    "sns.histplot(summary_token_counts, ax=ax2, color='green', alpha=0.7)\n",
    "ax2.set_title(\"Distribution of Summary Token Counts\", fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel(\"Number of Tokens\", fontsize=12)\n",
    "ax2.set_ylabel(\"Frequency\", fontsize=12)\n",
    "ax2.grid(axis='y', alpha=0.5)\n",
    "\n",
    "plt.suptitle(\"Token Count Distributions\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc579a-0b0e-4996-8075-a6eef56caf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 3\n",
    "BATCH_SIZE= 16\n",
    "\n",
    "data_module = NewsDataModule(\n",
    "    train_df, \n",
    "    test_df,\n",
    "    tokenizer,\n",
    "    batch_size=BATCH_SIZE\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6e5563-b45a-40c7-978d-8e6a65629536",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, decoder_attention_mask, labels=None):\n",
    "        output = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            decoder_attention_mask=decoder_attention_mask\n",
    "        )\n",
    "        return output.loss, output.logits\n",
    "\n",
    "    def shared_step(self, batch, batch_idx, stage):\n",
    "        input_ids = batch['text_input_ids']\n",
    "        attention_mask = batch[\"text_attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        labels_attention_mask = batch[\"labels_attention_mask\"]\n",
    "\n",
    "        loss, _ = self(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_attention_mask=labels_attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        self.log(f\"{stage}_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, batch_idx, 'train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, batch_idx, 'val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, batch_idx, 'test')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8787b13-b035-4cd6-ab0b-412aab0766f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = SummaryModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f28dee-2ca0-4f0c-93bb-1b38f415a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lighting_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cdf348-ca50-4807-ba09-254ce567cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"base-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"news_summary\")\n",
    "\n",
    "trainer= Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=callbacks,\n",
    "    max_epochs=N_EPOCHS,\n",
    "    accelerator = 'cpu'\n",
    "    #gpus=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236b7b85-939b-4468-b124-046f9fcc3391",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model_1, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e317dc-9f51-461e-a9fb-d01bf07454dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = SummaryModel.load_from_checkpoint(\n",
    "    trainer.checkpoint_callback.best_model_path\n",
    ")\n",
    "best_model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0403fcb-6c6e-4a1d-8abe-64c919d35741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = open('text_summarization_model.pkl', 'wb')\n",
    "pickle.dump(best_model.model, filename)\n",
    "model = pickle.load(open('text_summarization_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9749da3d-c749-43d0-ab3a-731fe3377474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text):\n",
    "    # Encode the text using the tokenizer\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
    "\n",
    "def generate_summary(input_ids, attention_mask):\n",
    "    # Generate a summary using the best model\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=150,\n",
    "        num_beams=2,\n",
    "        repetition_penalty=2.5,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return generated_ids\n",
    "\n",
    "def decode_summary(generated_ids):\n",
    "    # Decode the generated summary\n",
    "    summary = [tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "               for gen_id in generated_ids]\n",
    "    return \"\".join(summary)\n",
    "\n",
    "def summarize(text):\n",
    "    input_ids, attention_mask = encode_text(text)\n",
    "    generated_ids = generate_summary(input_ids, attention_mask)\n",
    "    summary = decode_summary(generated_ids)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2929e039-c0a2-4bc1-93ee-b8e58bbe230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary = summarize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe13efad-9992-4cfc-854c-9734d3e6cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Delhi Capitals’ head coach Ricky Ponting during a press conference in Delhi on Friday. | Photo Credit: PTI\n",
    "\n",
    "Ricky Ponting knows a thing or two about cricket and spotlight and how together, the two can either be a recipe for unprecedented success or unmitigated disaster, depending on how one handles them.\n",
    "\n",
    "In India, in particular, the pressure to manage both is a lot more than anywhere else and the IPL is at the pinnacle of fan attention. “Well it is a lot different in our country than it is here. The big thing about the IPL is seeing so many younger players getting an opportunity that they are not ready for. And I don’t mean the sport per se. They are ready for the cricket side of it but there are a lot of guys not ready, yet, for the many other things that come with cricket. There wasn’t as much spotlight on me back as a young player as on some of the young Indian players today,” Ponting admitted.\n",
    "\n",
    "And, being the coach and legend that he is, he accepts the responsibility to try and guide them. “For me, it’s letting players understand how big what they are doing actually is, in the public’s eyes. As a player you want to play cricket, you want to represent your team and franchise and country, but sometimes you can’t see the bigger picture behind it than just you playing cricket. It’s also about how everyone sees you in the real world and the IPL, for a lot of these youngsters, is not the real world. There’s a lot of other stuff happening out there,” he cautioned.\n",
    "\n",
    "\n",
    "Ponting predicts this year’s IPL will see the real Prithvi Shaw\n",
    "His advice? Get your act together outside the field so you can perform inside. “My job is to make them better players but, at the end of the day, I want them to be better people. I think the better you are as a person, the easier it is to be a better player and if you haven’t got your life in order off the field, it’s really difficult to be a disciplined performer on it. That’s one of the things I try to teach because I have been there, done that,” the 48-year-old World Cup-winning captain explained.\n",
    "\n",
    "And in a World Cup year, who better than a two-time winning captain to ask about the constant hype around Indian performers in IPL? “Ideally we would want them all to have that drive and passion to be the best they can be but the one thing I always stress with young guys is, not to start looking too far ahead and thinking about the World Cup. They need to stay in the present and think about the here and now and play their role in the team. My job is to train and get these guys ready to win games for us but after that, their selections for any other event or format are not in my hands,” he shrugged.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b5ae5-4c13-4019-afa6-c2ab9c29d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_row[\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978975b0-7c86-4c45-8628-e4ed872c2a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
