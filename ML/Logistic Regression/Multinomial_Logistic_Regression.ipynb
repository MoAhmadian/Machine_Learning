{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a48553c9-6b2b-4d76-80db-783528f2a9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10) (1000,)\n",
      "Counter({1: 334, 2: 334, 0: 332})\n"
     ]
    }
   ],
   "source": [
    "# Generate test classification dataset with 1000 rows \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)\n",
    "print(Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a77c78f-b818-4141-8450-e80227d5d8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(multi_class='multinomial')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The LogisticRegression class can be configured for multinomial logistic regression by setting the “multi_class” argument to “multinomial” and the “solver” argument \n",
    "#to a solver that supports multinomial logistic regression, such as “lbfgs“.\n",
    "# define the multinomial logistic regression model\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a542225-f60a-4d85-9f67-06c6b4a8aeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.681 (0.042)\n"
     ]
    }
   ],
   "source": [
    "# define the model evaluation procedure\n",
    "# We will use three repeats with 10 folds, which is a good default, and evaluate model performance using classification accuracy given that the classes are balanced.\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model and collect the scores\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report the model performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffd72181-4012-41b1-9731-9d7a2915b20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 1\n"
     ]
    }
   ],
   "source": [
    "# Prediction using trained model\n",
    "# define a single row of input data\n",
    "row = [1.89149379, -0.39847585, 1.63856893, 0.01647165, 1.51892395, -3.52651223, 1.80998823, 0.58810926, -0.02542177, -0.52835426]\n",
    "# predict the class label\n",
    "yhat = model.predict([row])\n",
    "# summarize the predicted class\n",
    "print('Predicted Class: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a7c25f6-b414-40b2-82de-506955ad9013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Probabilities: [0.16470456 0.50297138 0.33232406]\n"
     ]
    }
   ],
   "source": [
    "# A benefit of multinomial logistic regression is that it can predict calibrated probabilities across all known class labels in the dataset.\n",
    "#This can be achieved by calling the predict_proba() function on the model.\n",
    "yhat = model.predict_proba([row])\n",
    "# summarize the predicted probabilities\n",
    "print('Predicted Probabilities: %s' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02b5d83-e7ce-47ed-8fb9-f7c4983f1bdd",
   "metadata": {},
   "source": [
    "## Tune Penalty for Multinomial Logistic Regression\n",
    "\n",
    "An important hyperparameter to tune for multinomial logistic regression is the penalty term. This term imposes pressure on the model to seek smaller model weights. This is achieved by adding a weighted sum of the model coefficients to the loss function, encouraging the model to reduce the size of the weights along with the error while fitting the model.\n",
    "\n",
    "A popular type of penalty is the L2 penalty that adds the (weighted) sum of the squared coefficients to the loss function. A weighting of the coefficients can be used that reduces the strength of the penalty from full penalty to a very slight penalty.\n",
    "\n",
    "By default, the LogisticRegression class uses the L2 penalty with a weighting of coefficients set to 1.0. The type of penalty can be set via the “penalty” argument with values of “l1“, “l2“, “elasticnet” (e.g. both), although not all solvers support all penalty types. The weighting of the coefficients in the penalty can be set via the “C” argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9841acc3-2533-4041-87e6-b84641d111f7",
   "metadata": {},
   "source": [
    " define the multinomial logistic regression model with a default penalty\n",
    "LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059055ab-ce7a-458d-97be-cccc26203238",
   "metadata": {},
   "source": [
    "The weighting for the penalty is actually the inverse weighting, perhaps penalty = 1 – C.\n",
    "\n",
    "C : float, default=1.0\n",
    "Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
    "\n",
    "\n",
    "This means that values close to 1.0 indicate very little penalty and values close to zero indicate a strong penalty. A C value of 1.0 may indicate no penalty at all.\n",
    "\n",
    "C close to 1.0: Light penalty.\n",
    "C close to 0.0: Strong penalty.\n",
    "The penalty can be disabled by setting the “penalty” argument to the string “none“.\n",
    "\n",
    "We will explore the L2 penalty with weighting values in the range from 0.0001 to 1.0 on a log scale, in addition to no penalty or 0.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76e433d7-0742-4803-a788-de525dafe647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">0.0000 0.777 (0.037)\n",
      ">0.0001 0.683 (0.049)\n",
      ">0.0010 0.762 (0.044)\n",
      ">0.0100 0.775 (0.040)\n",
      ">0.1000 0.774 (0.038)\n",
      ">1.0000 0.777 (0.037)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the dataset\n",
    "def get_dataset():\n",
    "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1, n_classes=3)\n",
    "    return X, y\n",
    " \n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    for p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
    "    # create name for model\n",
    "        key = '%.4f' % p\n",
    "        #turn off penalty in some cases\n",
    "        if p == 0.0:\n",
    "            # no penalty in this case\n",
    "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none')\n",
    "        else:\n",
    "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p)\n",
    "    return models\n",
    "\n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    # define the evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # evaluate the model\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    # evaluate the model and collect the scores\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    # store the results\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    # summarize progress along the way\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()\n",
    "# In this case, we can see that a C value of 1.0 has the best score of about 77.7 percent, \n",
    "#which is the same as using no penalty that achieves the same score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1746d66e-ba6b-4319-9b49-f5864343ac17",
   "metadata": {},
   "source": [
    "### Conclusion: A box and whisker plot is created for the accuracy scores for each configuration and all plots are shown side by side on a figure on the same scale for direct comparison.\n",
    "\n",
    "In this case, we can see that the larger penalty we use on this dataset (i.e. the smaller the C value), the worse the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957acf34-6ff4-4625-b4f5-ebda0f790d22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
